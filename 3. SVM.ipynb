{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da37291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ 1.  1.  1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleSVM:\n",
    "    def __init__(self, C=1.0, max_iter=1000, epsilon=1e-3, kernel='linear'):\n",
    "        self.C = C  # Regularization parameter\n",
    "        self.max_iter = max_iter  # Maximum number of iterations\n",
    "        self.epsilon = epsilon  # Convergence criterion\n",
    "        self.kernel = kernel\n",
    "        self.alpha = None\n",
    "        self.b = 0\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def kernel_function(self, x1, x2):\n",
    "        if self.kernel == 'linear':\n",
    "            return np.dot(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(\"Kernel type not supported\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        n_samples, n_features = X.shape\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            alpha_prev = np.copy(self.alpha)\n",
    "            for j in range(n_samples):\n",
    "                i = self.select_second_alpha(j, n_samples)\n",
    "                x_i, x_j, y_i, y_j = X[i, :], X[j, :], y[i], y[j]\n",
    "                k_ij = self.kernel_function(x_i, x_i) + self.kernel_function(x_j, x_j) - 2 * self.kernel_function(x_i, x_j)\n",
    "                if k_ij == 0:\n",
    "                    continue\n",
    "                alpha_j_unc = self.alpha[j] + y_j * (self.error(i) - self.error(j)) / k_ij\n",
    "                self.alpha[j] = self.clip_alpha(alpha_j_unc, y_j, y_i, i, j)\n",
    "                self.alpha[i] += y_i * y_j * (alpha_prev[j] - self.alpha[j])\n",
    "\n",
    "                b1 = self.b - self.error(i) - y_i * (self.alpha[i] - alpha_prev[i]) * self.kernel_function(x_i, x_i) - y_j * (self.alpha[j] - alpha_prev[j]) * self.kernel_function(x_i, x_j)\n",
    "                b2 = self.b - self.error(j) - y_i * (self.alpha[i] - alpha_prev[i]) * self.kernel_function(x_i, x_j) - y_j * (self.alpha[j] - alpha_prev[j]) * self.kernel_function(x_j, x_j)\n",
    "                self.b = (b1 + b2) / 2\n",
    "\n",
    "                if self.converged(alpha_prev):\n",
    "                    break\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.dot((self.alpha * self.y), self.X.dot(X.T)) + self.b\n",
    "        return np.sign(y_pred)\n",
    "\n",
    "    def error(self, i):\n",
    "        return self.predict(self.X[i]) - self.y[i]\n",
    "\n",
    "    def select_second_alpha(self, index, n_samples):\n",
    "        if n_samples > 1:\n",
    "            l = list(range(n_samples))\n",
    "            l.remove(index)\n",
    "            return np.random.choice(l)\n",
    "        return 0\n",
    "\n",
    "    def clip_alpha(self, alpha_j, y_j, y_i, i, j):\n",
    "        L = max(0, self.alpha[j] - self.alpha[i]) if y_i != y_j else max(0, self.alpha[i] + self.alpha[j] - self.C)\n",
    "        H = min(self.C, self.C + self.alpha[j] - self.alpha[i]) if y_i != y_j else min(self.C, self.alpha[i] + self.alpha[j])\n",
    "        return max(L, min(alpha_j, H))\n",
    "\n",
    "    def converged(self, alpha_prev):\n",
    "        diff = np.linalg.norm(self.alpha - alpha_prev)\n",
    "        return diff < self.epsilon\n",
    "\n",
    "# Example usage with a toy dataset\n",
    "if __name__ == '__main__':\n",
    "    # Toy dataset: simple linearly separable\n",
    "    X_train = np.array([[1, 2], [2, 3], [3, 3], [2, 1], [3, 2]])\n",
    "    y_train = np.array([1, 1, 1, -1, -1])\n",
    "\n",
    "    svm = SimpleSVM()\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = svm.predict(X_train)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7f2cc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'A' must be a 'd' matrix with 80 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-90cdd42df0cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m  \u001b[0;31m# Parameter for the RBF kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_vector_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_vector_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# Make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-90cdd42df0cc>\u001b[0m in \u001b[0;36mtrain_svm\u001b[0;34m(X, y, C, kernel, gamma)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Solve Quadratic Optimization Problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquadratic_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Support Vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-90cdd42df0cc>\u001b[0m in \u001b[0;36mquadratic_optimizer\u001b[0;34m(P, q, G, h, A, b)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure A is a 2D matrix with a single row and columns equal to the number of samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure b is a 2D matrix with a single element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/cvxopt/coneprog.py\u001b[0m in \u001b[0;36mqp\u001b[0;34m(P, q, G, h, A, b, solver, kktsolver, initvals, **kwargs)\u001b[0m\n\u001b[1;32m   4483\u001b[0m             'residual as dual infeasibility certificate': dinfres}\n\u001b[1;32m   4484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4485\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconeqp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkktsolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/cvxopt/coneprog.py\u001b[0m in \u001b[0;36mconeqp\u001b[0;34m(P, q, G, h, dims, A, b, initvals, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatrixA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypecode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'd'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m             raise TypeError(\"'A' must be a 'd' matrix with %d columns\" \\\n\u001b[0m\u001b[1;32m   1914\u001b[0m                 %q.size[0])\n\u001b[1;32m   1915\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'A' must be a 'd' matrix with 80 columns"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# **1. Kernel Functions**\n",
    "def linear_kernel(X1, X2):\n",
    "    return np.dot(X1, X2.T)\n",
    "\n",
    "def rbf_kernel(X1, X2, gamma):\n",
    "    sq_dist = np.sum(X1**2, axis=1)[:, None] + np.sum(X2**2, axis=1)[None, :] - 2 * np.dot(X1, X2.T)\n",
    "    return np.exp(-gamma * sq_dist)\n",
    "\n",
    "# **2. Hypothetical Quadratic Optimization using CVXOPT**\n",
    "def quadratic_optimizer(P, q, G, h, A, b):\n",
    "    P = matrix(P)\n",
    "    q = matrix(q)\n",
    "    G = matrix(G)\n",
    "    h = matrix(h)\n",
    "    A = matrix(A, (1, len(q)))  # Ensure A is a 2D matrix with a single row and columns equal to the number of samples\n",
    "    b = matrix(b)  # Ensure b is a 2D matrix with a single element\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    alphas = np.array(solution['x']).flatten()\n",
    "    return alphas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **3. SVM Training**\n",
    "def train_svm(X, y, C, kernel='rbf', gamma=0.1):\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Build Kernel Matrix\n",
    "    if kernel == 'rbf':\n",
    "        K = rbf_kernel(X, X, gamma) \n",
    "    else:  \n",
    "        K = linear_kernel(X, X) \n",
    "\n",
    "    # Setup Quadratic Optimization Problem\n",
    "    P = np.outer(y, y) * K\n",
    "    q = -np.ones(n_samples)\n",
    "    G = np.vstack([-np.eye(n_samples), np.eye(n_samples)])\n",
    "    h = np.hstack([np.zeros(n_samples), C * np.ones(n_samples)])\n",
    "    A = y[np.newaxis, :]  # Equality constraint \n",
    "    b = np.array([0]) \n",
    "\n",
    "    # Solve Quadratic Optimization Problem\n",
    "    alphas = quadratic_optimizer(P, q, G, h, A, b)\n",
    "\n",
    "    # Support Vectors\n",
    "    support_vector_indices = np.where(alphas > 1e-5)[0]\n",
    "    support_vectors = X[support_vector_indices]\n",
    "    support_vector_labels = y[support_vector_indices]\n",
    "\n",
    "    return alphas, support_vector_indices, support_vectors, support_vector_labels\n",
    "\n",
    "# **4. SVM Prediction**\n",
    "def predict(X, alphas, support_vectors, support_vector_labels, kernel='rbf', gamma=0.1):\n",
    "    if kernel == 'rbf':\n",
    "        k = rbf_kernel(X, support_vectors, gamma)  \n",
    "    else:\n",
    "        k = linear_kernel(X, support_vectors)\n",
    "    predictions = np.sign(np.dot(k, alphas[support_vector_indices] * support_vector_labels))\n",
    "    return predictions\n",
    "\n",
    "# **5. Evaluation Metrics**\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    predicted_positives = np.sum(y_pred == 1)\n",
    "    return true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    actual_positives = np.sum(y_true == 1)\n",
    "    return true_positives / actual_positives if actual_positives > 0 else 0\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=6, n_features=2)\n",
    "\n",
    "# Convert labels to {-1, 1}\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the SVM\n",
    "C = 1  # Regularization parameter\n",
    "kernel = 'rbf'  # Kernel type ('rbf' or 'linear')\n",
    "gamma = 0.1  # Parameter for the RBF kernel\n",
    "\n",
    "alphas, support_vector_indices, support_vectors, support_vector_labels = train_svm(X_train_scaled, y_train, C, kernel, gamma)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = predict(X_test_scaled, alphas, support_vectors, support_vector_labels, kernel, gamma)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_val = accuracy(y_test, y_pred)\n",
    "precision_val = precision(y_test, y_pred)\n",
    "recall_val = recall(y_test, y_pred)\n",
    "f1_score_val = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_val}\")\n",
    "print(f\"Precision: {precision_val}\")\n",
    "print(f\"Recall: {recall_val}\")\n",
    "print(f\"F1 Score: {f1_score_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fff36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc3cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
