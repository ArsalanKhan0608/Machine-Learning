{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd13786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris  # Sample dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    # Assuming binary classification for this example\n",
    "    y = (y == 0).astype(int)  # Target class 0 vs. others\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient descent with optimizations\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            # Gradients (vectorized for efficiency)\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]  # Threshold\n",
    "        return y_predicted_cls\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    if tp + fp == 0:\n",
    "        return 0  # or return None or np.nan if you prefer\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    if tp + fn == 0:\n",
    "        return 0  # or return None or np.nan if you prefer\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c72b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 0\n",
      "Recall: 0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_and_preprocess_data()\n",
    "\n",
    "model = LogisticRegression(learning_rate=0.1, num_iterations=1500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy(y_test, y_pred))\n",
    "print(\"Precision:\", precision(y_test, y_pred))\n",
    "print(\"Recall:\", recall(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f800e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100.00%\n",
      "Optimized weights: [-0.03967765  1.78988091 -0.5725503 ]\n",
      "Cost history: [0.69108522 0.68905647 0.68704086 0.68503828 0.68304864 0.68107186\n",
      " 0.67910784 0.67715649 0.67521771 0.67329143 0.67137754 0.66947597\n",
      " 0.66758661 0.66570939 0.6638442  0.66199098 0.66014962 0.65832004\n",
      " 0.65650215 0.65469587 0.65290112 0.6511178  0.64934583 0.64758513\n",
      " 0.64583561 0.64409719 0.64236979 0.64065333 0.63894771 0.63725287\n",
      " 0.63556871 0.63389517 0.63223215 0.63057958 0.62893739 0.62730548\n",
      " 0.62568379 0.62407223 0.62247073 0.62087921 0.6192976  0.61772581\n",
      " 0.61616379 0.61461144 0.6130687  0.61153549 0.61001174 0.60849737\n",
      " 0.60699232 0.60549651 0.60400988 0.60253234 0.60106383 0.59960429\n",
      " 0.59815364 0.59671181 0.59527874 0.59385435 0.59243858 0.59103137\n",
      " 0.58963265 0.58824234 0.5868604  0.58548674 0.58412132 0.58276406\n",
      " 0.5814149  0.58007377 0.57874063 0.57741539 0.57609801 0.57478843\n",
      " 0.57348657 0.57219239 0.57090582 0.5696268  0.56835528 0.5670912\n",
      " 0.56583449 0.56458511 0.56334299 0.56210809 0.56088034 0.55965968\n",
      " 0.55844608 0.55723946 0.55603977 0.55484697 0.553661   0.55248181\n",
      " 0.55130934 0.55014354 0.54898437 0.54783176 0.54668568 0.54554607\n",
      " 0.54441287 0.54328605 0.54216555 0.54105132 0.53994333 0.5388415\n",
      " 0.53774581 0.53665621 0.53557264 0.53449507 0.53342344 0.53235771\n",
      " 0.53129784 0.53024378 0.52919549 0.52815292 0.52711603 0.52608478\n",
      " 0.52505913 0.52403903 0.52302443 0.52201531 0.52101162 0.52001331\n",
      " 0.51902035 0.5180327  0.51705031 0.51607315 0.51510118 0.51413435\n",
      " 0.51317264 0.512216   0.5112644  0.51031779 0.50937614 0.50843942\n",
      " 0.50750758 0.5065806  0.50565843 0.50474103 0.50382839 0.50292045\n",
      " 0.50201718 0.50111856 0.50022454 0.49933509 0.49845018 0.49756977\n",
      " 0.49669384 0.49582235 0.49495526 0.49409255 0.49323418 0.49238012\n",
      " 0.49153035 0.49068482 0.48984351 0.48900639 0.48817343 0.4873446\n",
      " 0.48651986 0.48569919 0.48488257 0.48406995 0.48326131 0.48245663\n",
      " 0.48165587 0.48085901 0.48006602 0.47927688 0.47849154 0.47771\n",
      " 0.47693221 0.47615816 0.47538782 0.47462116 0.47385816 0.47309878\n",
      " 0.47234301 0.47159082 0.47084219 0.47009708 0.46935548 0.46861736\n",
      " 0.4678827  0.46715147 0.46642365 0.46569921 0.46497813 0.4642604\n",
      " 0.46354598 0.46283485 0.46212699 0.46142238 0.460721   0.46002282\n",
      " 0.45932782 0.45863598 0.45794728 0.45726169 0.4565792  0.45589979\n",
      " 0.45522343 0.45455011 0.4538798  0.45321248 0.45254813 0.45188674\n",
      " 0.45122828 0.45057273 0.44992008 0.4492703  0.44862338 0.44797929\n",
      " 0.44733802 0.44669956 0.44606387 0.44543094 0.44480076 0.44417331\n",
      " 0.44354856 0.4429265  0.44230712 0.44169039 0.4410763  0.44046483\n",
      " 0.43985597 0.43924969 0.43864598 0.43804482 0.43744621 0.43685011\n",
      " 0.43625652 0.43566541 0.43507678 0.43449061 0.43390687 0.43332556\n",
      " 0.43274666 0.43217016 0.43159603 0.43102427 0.43045486 0.42988778\n",
      " 0.42932302 0.42876057 0.42820041 0.42764252 0.4270869  0.42653352\n",
      " 0.42598238 0.42543345 0.42488674 0.42434221 0.42379986 0.42325968\n",
      " 0.42272165 0.42218576 0.421652   0.42112034 0.42059079 0.42006332\n",
      " 0.41953793 0.41901459 0.41849331 0.41797406 0.41745683 0.41694162\n",
      " 0.4164284  0.41591717 0.41540792 0.41490063 0.41439529 0.41389189\n",
      " 0.41339042 0.41289086 0.41239321 0.41189746 0.41140358 0.41091158\n",
      " 0.41042143 0.40993314 0.40944668 0.40896205 0.40847924 0.40799823\n",
      " 0.40751902 0.40704159 0.40656594 0.40609205 0.40561991 0.40514952\n",
      " 0.40468086 0.40421392 0.40374869 0.40328517 0.40282335 0.4023632\n",
      " 0.40190473 0.40144792 0.40099277 0.40053927 0.4000874  0.39963715\n",
      " 0.39918852 0.39874151 0.39829609 0.39785226 0.39741001 0.39696933\n",
      " 0.39653021 0.39609265 0.39565664 0.39522216 0.39478921 0.39435778\n",
      " 0.39392786 0.39349945 0.39307253 0.39264709 0.39222313 0.39180064\n",
      " 0.39137962 0.39096005 0.39054192 0.39012523 0.38970997 0.38929613\n",
      " 0.38888371 0.38847269 0.38806307 0.38765484 0.387248   0.38684253\n",
      " 0.38643843 0.38603569 0.38563431 0.38523427 0.38483557 0.38443821\n",
      " 0.38404216 0.38364744 0.38325403 0.38286192 0.38247111 0.38208159\n",
      " 0.38169335 0.38130639 0.3809207  0.38053627 0.38015309 0.37977117\n",
      " 0.37939049 0.37901105 0.37863284 0.37825585 0.37788007 0.37750551\n",
      " 0.37713216 0.37676    0.37638904 0.37601926 0.37565066 0.37528323\n",
      " 0.37491698 0.37455188 0.37418794 0.37382515 0.37346351 0.373103\n",
      " 0.37274363 0.37238538 0.37202826 0.37167225 0.37131734 0.37096355\n",
      " 0.37061085 0.37025924 0.36990873 0.36955929 0.36921093 0.36886365\n",
      " 0.36851742 0.36817226 0.36782816 0.3674851  0.36714309 0.36680212\n",
      " 0.36646219 0.36612328 0.3657854  0.36544854 0.36511269 0.36477785\n",
      " 0.36444402 0.36411118 0.36377934 0.36344849 0.36311863 0.36278974\n",
      " 0.36246183 0.36213489 0.36180892 0.36148391 0.36115986 0.36083676\n",
      " 0.3605146  0.36019339 0.35987312 0.35955378 0.35923538 0.35891789\n",
      " 0.35860133 0.35828569 0.35797095 0.35765713 0.35734421 0.35703218\n",
      " 0.35672106 0.35641082 0.35610147 0.35579301 0.35548542 0.3551787\n",
      " 0.35487286 0.35456788 0.35426377 0.35396051 0.35365811 0.35335656\n",
      " 0.35305585 0.35275599 0.35245696 0.35215877 0.35186141 0.35156488\n",
      " 0.35126917 0.35097429 0.35068021 0.35038695 0.3500945  0.34980285\n",
      " 0.34951201 0.34922196 0.34893271 0.34864424 0.34835657 0.34806967\n",
      " 0.34778356 0.34749822 0.34721366 0.34692986 0.34664683 0.34636457\n",
      " 0.34608306 0.34580231 0.34552231 0.34524306 0.34496456 0.3446868\n",
      " 0.34440978 0.34413349 0.34385794 0.34358312 0.34330902 0.34303565\n",
      " 0.34276299 0.34249106 0.34221984 0.34194933 0.34167952 0.34141042\n",
      " 0.34114203 0.34087433 0.34060733 0.34034102 0.3400754  0.33981047\n",
      " 0.33954622 0.33928265 0.33901976 0.33875754 0.338496   0.33823513\n",
      " 0.33797492 0.33771538 0.33745649 0.33719827 0.3369407  0.33668378\n",
      " 0.33642752 0.3361719  0.33591693 0.33566259 0.3354089  0.33515584\n",
      " 0.33490342 0.33465163 0.33440046 0.33414993 0.33390001 0.33365072\n",
      " 0.33340205 0.33315399 0.33290654 0.33265971 0.33241348 0.33216786\n",
      " 0.33192284 0.33167842 0.33143461 0.33119138 0.33094875 0.33070672\n",
      " 0.33046527 0.3302244  0.32998413 0.32974443 0.32950531 0.32926677\n",
      " 0.3290288  0.32879141 0.32855459 0.32831833 0.32808264 0.32784751\n",
      " 0.32761295 0.32737894 0.32714549 0.32691259 0.32668025 0.32644846\n",
      " 0.32621721 0.32598651 0.32575636 0.32552674 0.32529767 0.32506913\n",
      " 0.32484112 0.32461366 0.32438672 0.32416031 0.32393442 0.32370907\n",
      " 0.32348423 0.32325992 0.32303612 0.32281284 0.32259008 0.32236783\n",
      " 0.32214609 0.32192486 0.32170414 0.32148392 0.3212642  0.32104499\n",
      " 0.32082627 0.32060806 0.32039033 0.32017311 0.31995637 0.31974012\n",
      " 0.31952437 0.31930909 0.31909431 0.31888    0.31866618 0.31845283\n",
      " 0.31823997 0.31802758 0.31781566 0.31760421 0.31739324 0.31718273\n",
      " 0.31697269 0.31676311 0.316554   0.31634535 0.31613716 0.31592943\n",
      " 0.31572215 0.31551533 0.31530896 0.31510305 0.31489758 0.31469256\n",
      " 0.31448799 0.31428387 0.31408018 0.31387694 0.31367414 0.31347178\n",
      " 0.31326986 0.31306837 0.31286731 0.31266669 0.3124665  0.31226673\n",
      " 0.3120674  0.31186849 0.31167001 0.31147194 0.31127431 0.31107709\n",
      " 0.31088029 0.3106839  0.31048794 0.31029238 0.31009724 0.30990251\n",
      " 0.30970819 0.30951428 0.30932078 0.30912768 0.30893499 0.3087427\n",
      " 0.30855081 0.30835932 0.30816823 0.30797753 0.30778724 0.30759733\n",
      " 0.30740782 0.3072187  0.30702998 0.30684164 0.30665369 0.30646612\n",
      " 0.30627894 0.30609215 0.30590573 0.3057197  0.30553405 0.30534877\n",
      " 0.30516388 0.30497936 0.30479521 0.30461144 0.30442804 0.30424501\n",
      " 0.30406235 0.30388006 0.30369813 0.30351657 0.30333538 0.30315455\n",
      " 0.30297408 0.30279398 0.30261423 0.30243484 0.30225581 0.30207714\n",
      " 0.30189882 0.30172085 0.30154324 0.30136598 0.30118907 0.30101251\n",
      " 0.30083629 0.30066043 0.30048491 0.30030973 0.3001349  0.29996041\n",
      " 0.29978626 0.29961245 0.29943899 0.29926586 0.29909306 0.2989206\n",
      " 0.29874848 0.29857669 0.29840523 0.29823411 0.29806331 0.29789285\n",
      " 0.29772271 0.2975529  0.29738342 0.29721426 0.29704542 0.29687691\n",
      " 0.29670872 0.29654085 0.29637331 0.29620608 0.29603917 0.29587257\n",
      " 0.29570629 0.29554033 0.29537468 0.29520934 0.29504432 0.29487961\n",
      " 0.2947152  0.29455111 0.29438732 0.29422384 0.29406067 0.2938978\n",
      " 0.29373524 0.29357298 0.29341102 0.29324937 0.29308801 0.29292695\n",
      " 0.2927662  0.29260574 0.29244557 0.29228571 0.29212613 0.29196686\n",
      " 0.29180787 0.29164918 0.29149078 0.29133267 0.29117485 0.29101731\n",
      " 0.29086007 0.29070311 0.29054644 0.29039006 0.29023395 0.29007814\n",
      " 0.2899226  0.28976735 0.28961237 0.28945768 0.28930327 0.28914913\n",
      " 0.28899527 0.28884169 0.28868839 0.28853536 0.2883826  0.28823012\n",
      " 0.28807791 0.28792597 0.28777431 0.28762291 0.28747178 0.28732092\n",
      " 0.28717033 0.28702001 0.28686995 0.28672016 0.28657063 0.28642137\n",
      " 0.28627237 0.28612363 0.28597515 0.28582694 0.28567898 0.28553128\n",
      " 0.28538385 0.28523667 0.28508974 0.28494308 0.28479667 0.28465051\n",
      " 0.28450461 0.28435896 0.28421356 0.28406842 0.28392352 0.28377888\n",
      " 0.28363449 0.28349034 0.28334644 0.2832028  0.28305939 0.28291624\n",
      " 0.28277333 0.28263066 0.28248824 0.28234606 0.28220413 0.28206243\n",
      " 0.28192098 0.28177977 0.2816388  0.28149806 0.28135757 0.28121731\n",
      " 0.28107729 0.28093751 0.28079797 0.28065865 0.28051958 0.28038073\n",
      " 0.28024212 0.28010375 0.2799656  0.27982769 0.27969    0.27955255\n",
      " 0.27941533 0.27927833 0.27914157 0.27900503 0.27886871 0.27873263\n",
      " 0.27859677 0.27846113 0.27832572 0.27819053 0.27805557 0.27792082\n",
      " 0.2777863  0.277652   0.27751793 0.27738407 0.27725043 0.27711701\n",
      " 0.27698381 0.27685082 0.27671806 0.27658551 0.27645317 0.27632105\n",
      " 0.27618915 0.27605746 0.27592598 0.27579472 0.27566367 0.27553283\n",
      " 0.2754022  0.27527178 0.27514158 0.27501158 0.27488179 0.27475221\n",
      " 0.27462284 0.27449368 0.27436472 0.27423597 0.27410742 0.27397908\n",
      " 0.27385095 0.27372302 0.27359529 0.27346776 0.27334044 0.27321332\n",
      " 0.2730864  0.27295968 0.27283317 0.27270685 0.27258073 0.27245481\n",
      " 0.27232909 0.27220357 0.27207824 0.27195311 0.27182818 0.27170344\n",
      " 0.2715789  0.27145455 0.2713304  0.27120644 0.27108267 0.2709591\n",
      " 0.27083572 0.27071253 0.27058953 0.27046672 0.2703441  0.27022167\n",
      " 0.27009943 0.26997738 0.26985552 0.26973385 0.26961236 0.26949106\n",
      " 0.26936995 0.26924902 0.26912828 0.26900772 0.26888735 0.26876716\n",
      " 0.26864715 0.26852733 0.26840769 0.26828823 0.26816896 0.26804986\n",
      " 0.26793095 0.26781222 0.26769366 0.26757529 0.26745709 0.26733908\n",
      " 0.26722124 0.26710358 0.26698609 0.26686879 0.26675166 0.2666347\n",
      " 0.26651792 0.26640132 0.26628489 0.26616863 0.26605255 0.26593664\n",
      " 0.26582091 0.26570534 0.26558995 0.26547473 0.26535968 0.2652448\n",
      " 0.2651301  0.26501556 0.26490119 0.26478699 0.26467296 0.2645591\n",
      " 0.2644454  0.26433187 0.26421851 0.26410532 0.26399229 0.26387943\n",
      " 0.26376673 0.2636542  0.26354184 0.26342963 0.26331759 0.26320572\n",
      " 0.26309401 0.26298246 0.26287107 0.26275984 0.26264878 0.26253787\n",
      " 0.26242713 0.26231655 0.26220612 0.26209586 0.26198576 0.26187581\n",
      " 0.26176602 0.2616564  0.26154692 0.26143761 0.26132845 0.26121945\n",
      " 0.26111061 0.26100192 0.26089338 0.260785   0.26067678 0.26056871\n",
      " 0.26046079 0.26035303 0.26024542 0.26013797 0.26003066 0.25992351\n",
      " 0.25981651 0.25970966 0.25960296 0.25949642 0.25939002 0.25928377\n",
      " 0.25917768 0.25907173 0.25896593 0.25886028 0.25875478 0.25864943\n",
      " 0.25854422 0.25843916 0.25833425 0.25822948 0.25812486 0.25802039\n",
      " 0.25791606 0.25781188 0.25770784 0.25760395 0.2575002  0.25739659\n",
      " 0.25729313 0.25718981 0.25708664 0.2569836  0.25688071 0.25677796\n",
      " 0.25667535 0.25657289 0.25647056 0.25636838 0.25626633 0.25616443\n",
      " 0.25606266 0.25596104 0.25585955 0.2557582  0.255657   0.25555592\n",
      " 0.25545499 0.25535419 0.25525354 0.25515301]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Cost function for logistic regression\n",
    "def compute_cost(X, y, weights):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ weights)\n",
    "    epsilon = 1e-5  # To prevent log(0)\n",
    "    cost = (1/m) * ((-y).T @ np.log(h + epsilon) - (1 - y).T @ np.log(1 - h + epsilon))\n",
    "    return cost\n",
    "\n",
    "# Gradient descent to minimize the logistic regression cost function\n",
    "def gradient_descent(X, y, weights, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        weights = weights - (learning_rate/m) * (X.T @ (sigmoid(X @ weights) - y))\n",
    "        cost_history[i] = compute_cost(X, y, weights)\n",
    "\n",
    "    return weights, cost_history\n",
    "\n",
    "# Prediction function\n",
    "def predict(X, weights):\n",
    "    return sigmoid(X @ weights) >= 0.5\n",
    "\n",
    "# Accuracy calculation\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    return correct / len(y_true)\n",
    "\n",
    "# Data preparation (example with a simplified dataset)\n",
    "def prepare_data():\n",
    "    # Sample dataset: 10 samples with 2 features and a binary target\n",
    "    X = np.array([\n",
    "        [5.1, 3.5],\n",
    "        [4.9, 3.0],\n",
    "        [7.0, 3.2],\n",
    "        [6.4, 3.2],\n",
    "        [5.9, 3.0],\n",
    "        [5.4, 3.9],\n",
    "        [6.6, 2.9],\n",
    "        [5.6, 3.0],\n",
    "        [6.7, 3.1],\n",
    "        [5.6, 3.0]\n",
    "    ])\n",
    "    y = np.array([0, 0, 1, 1, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "    # Feature scaling\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    # Adding intercept term\n",
    "    m = X.shape[0]\n",
    "    X = np.hstack((np.ones((m, 1)), X))\n",
    "\n",
    "    # Splitting dataset into training (80%) and test (20%) sets\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    shuffle_indices = np.random.permutation(m)\n",
    "    test_size = int(m * 0.2)\n",
    "    test_indices = shuffle_indices[:test_size]\n",
    "    train_indices = shuffle_indices[test_size:]\n",
    "\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Main function to run the logistic regression model\n",
    "def logistic_regression(learning_rate=0.01, iterations=1000):\n",
    "    X_train, X_test, y_train, y_test = prepare_data()\n",
    "\n",
    "    # Initial weights (all zeros)\n",
    "    weights = np.zeros(X_train.shape[1])\n",
    "\n",
    "    # Running gradient descent to optimize weights\n",
    "    weights, cost_history = gradient_descent(X_train, y_train, weights, learning_rate, iterations)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_test = predict(X_test, weights)\n",
    "    test_accuracy = accuracy(y_test, y_pred_test)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "    return weights, cost_history\n",
    "\n",
    "# Run logistic regression\n",
    "weights, cost_history = logistic_regression()\n",
    "\n",
    "# Output weights and cost history for review\n",
    "print(\"Optimized weights:\", weights)\n",
    "print(\"Cost history:\", cost_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e152910a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
